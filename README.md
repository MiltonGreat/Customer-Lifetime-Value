# Customer-Lifetime-Value

### Summary and Recommendations

#### 1. Overview

The project involves using historical sales data to predict future customer value. By calculating RFM (Recency, Frequency, Monetary) metrics, the code builds models to predict CLV and then segments customers based on their predicted value into **Low**, **Medium**, and **High CLV** groups. The models include:

- Linear Regression
- Random Forest Regressor
- XGBoost Regressor

#### 2. Data

The dataset includes the following columns:

- **InvoiceNo**: Unique invoice number.
- **StockCode**: Product code.
- **Description**: Product description.
- **Quantity**: Number of units purchased.
- **InvoiceDate**: Date of the transaction.
- **UnitPrice**: Price of each unit.
- **CustomerID**: Unique identifier for each customer.
- **Country**: Country where the transaction occurred.
- **TotalPrice**: Total amount of the transaction (Quantity * UnitPrice).

The dataset is derived from an online retail store, containing over 390,000 rows of transaction data. Each transaction represents a purchase made by a customer on a particular date.

#### 3. Data Preprocessing

1. **Data Cleaning**: Handling missing values, removing negative quantities, and filtering outliers.
2. **Outlier Detection**: The top 1% of records for `Quantity`, `UnitPrice`, and `TotalPrice` were analyzed to remove extreme outliers.
3. **RFM Calculation**: Calculated **Recency**, **Frequency**, and **Monetary** metrics to quantify customer behavior.
4. **Date Features**: Additional features like `InvoiceMonth`, `InvoiceDayOfWeek`, and `InvoiceHour` were extracted from the date.

#### 4. Feature Engineering

- **Recency**: Days since the customer's last purchase.
- **Frequency**: Number of purchases made by the customer.
- **Monetary Value**: Total revenue generated by each customer.
- **Customer Segmentation**: Segmented customers using **KMeans** and **Agglomerative Clustering**.

#### 5.Models

The following models were trained and evaluated:

- **Linear Regression**
- **Random Forest Regressor**
- **XGBoost Regressor**

#### 6. Hyperparameter Tuning

Both Random Forest and XGBoost models were tuned using **RandomizedSearchCV** to optimize their hyperparameters for better performance.

#### 7. Evaluation

Models were evaluated based on:

- **Root Mean Squared Error (RMSE)**
- **Mean Absolute Error (MAE)**
- **R-squared (R²)**

#### 8. Key Findings
      
Model Performance:

- **Linear Regression**: RMSE: 836.26, R²: 0.99
- **Random Forest**: RMSE: 552.27, R²: 0.996
- **XGBoost**: RMSE: 4271.33, R²: 0.79

Insights

1. **Best Model**: The Random Forest Regressor outperformed the other models with the lowest RMSE and the highest R² value on the test set.
2. **Customer Segmentation**: Customers were segmented into **Low**, **Medium**, and **High CLV** groups based on predicted CLV values.
3. **XGBoost**: The XGBoost model underperformed compared to Random Forest, likely due to a mismatch in feature scaling or parameter tuning.

Results

- **High CLV Customers**: These customers have high predicted CLV values and are likely to generate the most revenue in the future.
- **Low CLV Customers**: These customers have the lowest predicted values and may need targeted promotions to increase their lifetime value.

#### 9. Source

https://www.kaggle.com/datasets/shreyanshverma27/online-sales-dataset-popular-marketplace-data
